{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ba03c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n",
      "Found 112 images belonging to 10 classes.\n",
      "Found 26 images belonging to 10 classes.\n",
      "Epoch 1/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 134ms/step - accuracy: 0.1462 - loss: 2.2926 - val_accuracy: 0.1538 - val_loss: 2.2484\n",
      "Epoch 2/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.2330 - loss: 2.2422 - val_accuracy: 0.1154 - val_loss: 2.2596\n",
      "Epoch 3/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.1486 - loss: 2.2323 - val_accuracy: 0.2308 - val_loss: 2.2558\n",
      "Epoch 4/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.1326 - loss: 2.2885 - val_accuracy: 0.1154 - val_loss: 2.2474\n",
      "Epoch 5/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.1821 - loss: 2.2110 - val_accuracy: 0.1154 - val_loss: 2.2501\n",
      "Epoch 6/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.1195 - loss: 2.2062 - val_accuracy: 0.1154 - val_loss: 2.2373\n",
      "Epoch 7/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.1999 - loss: 2.2074 - val_accuracy: 0.2308 - val_loss: 2.2291\n",
      "Epoch 8/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.1904 - loss: 2.2248 - val_accuracy: 0.1154 - val_loss: 2.2196\n",
      "Epoch 9/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 69ms/step - accuracy: 0.1838 - loss: 2.1855 - val_accuracy: 0.2308 - val_loss: 2.2081\n",
      "Epoch 10/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.2186 - loss: 2.1902 - val_accuracy: 0.3077 - val_loss: 2.1923\n",
      "Epoch 11/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.1639 - loss: 2.2153 - val_accuracy: 0.1538 - val_loss: 2.1770\n",
      "Epoch 12/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.1879 - loss: 2.1438 - val_accuracy: 0.1538 - val_loss: 2.1412\n",
      "Epoch 13/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 71ms/step - accuracy: 0.2342 - loss: 2.0896 - val_accuracy: 0.1538 - val_loss: 2.0902\n",
      "Epoch 14/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.2497 - loss: 2.0653 - val_accuracy: 0.3077 - val_loss: 2.0504\n",
      "Epoch 15/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - accuracy: 0.3101 - loss: 1.9958 - val_accuracy: 0.4615 - val_loss: 1.9513\n",
      "Epoch 16/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.3951 - loss: 1.8844 - val_accuracy: 0.5385 - val_loss: 1.8179\n",
      "Epoch 17/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.4261 - loss: 1.7797 - val_accuracy: 0.4615 - val_loss: 1.6793\n",
      "Epoch 18/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.4837 - loss: 1.6441 - val_accuracy: 0.5769 - val_loss: 1.4948\n",
      "Epoch 19/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.4869 - loss: 1.5029 - val_accuracy: 0.5769 - val_loss: 1.3966\n",
      "Epoch 20/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.4786 - loss: 1.4381 - val_accuracy: 0.3846 - val_loss: 1.3718\n",
      "Epoch 21/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5027 - loss: 1.4424 - val_accuracy: 0.6154 - val_loss: 1.1495\n",
      "Epoch 22/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.6652 - loss: 1.0852 - val_accuracy: 0.5000 - val_loss: 1.1054\n",
      "Epoch 23/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.6187 - loss: 0.9880 - val_accuracy: 0.6923 - val_loss: 1.0377\n",
      "Epoch 24/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.6902 - loss: 0.8739 - val_accuracy: 0.6538 - val_loss: 0.9283\n",
      "Epoch 25/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.7513 - loss: 0.8698 - val_accuracy: 0.7308 - val_loss: 0.8171\n",
      "Epoch 26/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.7417 - loss: 0.7590 - val_accuracy: 0.7692 - val_loss: 0.6949\n",
      "Epoch 27/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.7222 - loss: 0.6272 - val_accuracy: 0.8077 - val_loss: 0.6297\n",
      "Epoch 28/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 77ms/step - accuracy: 0.7629 - loss: 0.6266 - val_accuracy: 0.7692 - val_loss: 0.7124\n",
      "Epoch 29/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8712 - loss: 0.4436 - val_accuracy: 0.6923 - val_loss: 0.7431\n",
      "Epoch 30/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step - accuracy: 0.7896 - loss: 0.5616 - val_accuracy: 0.8462 - val_loss: 0.5100\n",
      "Epoch 31/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8708 - loss: 0.4192 - val_accuracy: 0.8077 - val_loss: 0.5279\n",
      "Epoch 32/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 74ms/step - accuracy: 0.8386 - loss: 0.4602 - val_accuracy: 0.8077 - val_loss: 0.4516\n",
      "Epoch 33/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.8788 - loss: 0.3344 - val_accuracy: 0.8462 - val_loss: 0.4467\n",
      "Epoch 34/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.9314 - loss: 0.2994 - val_accuracy: 0.8846 - val_loss: 0.3614\n",
      "Epoch 35/35\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step - accuracy: 0.9308 - loss: 0.2492 - val_accuracy: 0.9231 - val_loss: 0.3020\n",
      "Final Validation Accuracy: 0.9230769276618958\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "Predicted English digit: 5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Image properties\n",
    "IMG_SIZE = (64, 64)  # Resize images to 64x64\n",
    "NUM_CLASSES = 10# Digits 0-9\n",
    "\n",
    "# Define CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1)),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(NUM_CLASSES, activation=\"softmax\")  # Predicts digits (0-9)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "\n",
    "# Load dataset from directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\RUBAN\\Desktop\\mini project\\train data\",  # Update path to your dataset\n",
    "    target_size=IMG_SIZE, \n",
    "    color_mode=\"grayscale\", \n",
    "    batch_size=32, \n",
    "    class_mode=\"sparse\",\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\RUBAN\\Desktop\\mini project\\train data\",  # Update path to your dataset\n",
    "    target_size=IMG_SIZE, \n",
    "    color_mode=\"grayscale\", \n",
    "    batch_size=32, \n",
    "    class_mode=\"sparse\",\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# Train model\n",
    "# model.fit(train_generator, validation_data=val_generator, epochs=10)\n",
    "\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=35)\n",
    "# Print final validation accuracy\n",
    "print(\"Final Validation Accuracy:\", history.history[\"val_accuracy\"][-1])\n",
    "\n",
    "\n",
    "# Function to predict Tamil numeral images and return the English digit\n",
    "def predict_tamil_number(image_path):\n",
    "    img = Image.open(image_path).convert(\"L\").resize(IMG_SIZE)\n",
    "    img_array = np.array(img) / 255.0  # Normalize\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Reshape for model input\n",
    "    \n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_digit = np.argmax(prediction)  # Get the predicted English digit\n",
    "\n",
    "    return predicted_digit  # Return English number (0-9)\n",
    "\n",
    "# Test with an image\n",
    "image_path = r\"C:\\Users\\RUBAN\\Desktop\\mini project\\train data\\5\\5_001.jpg\"\n",
    "print(\"Predicted English digit:\", predict_tamil_number(image_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02841acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
